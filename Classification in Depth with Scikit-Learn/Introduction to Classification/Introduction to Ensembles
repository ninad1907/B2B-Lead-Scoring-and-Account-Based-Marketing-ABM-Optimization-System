Q1. What are ensemble models in machine learning?**  
A: Ensemble models combine multiple individual models to create a stronger model. The idea is that by aggregating the predictions of several models, the ensemble can reduce errors, variance, or bias, leading to more accurate and robust predictions. Common types of ensemble methods include bagging, boosting, and stacking.

---

Q2. What is the difference between bagging and boosting in ensemble learning?**  
A: 
- **Bagging (Bootstrap Aggregating)**: Involves training multiple models (usually of the same type) on different subsets of the data (generated by bootstrapping) and then averaging or voting on the predictions. Example: Random Forest.  
- **Boosting**: Involves training multiple models sequentially, with each model focusing on the errors made by the previous one. The final prediction is a weighted sum of the predictions. Example: AdaBoost, Gradient Boosting.

---

Q3. What is stacking in ensemble learning?**  
A: Stacking (or stacked generalization) involves training multiple different models on the same dataset and then using another model (called a meta-model) to combine the predictions of these base models. The meta-model learns to make the final prediction based on the outputs of the base models.
