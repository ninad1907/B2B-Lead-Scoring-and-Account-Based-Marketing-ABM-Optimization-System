Code:
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
data = {'fruit': ['apple', 'banana', 'apple', 'banana', 'banana','apple','apple','apple'],
        'color': ['red', 'yellow', 'green', 'yellow', 'yellow','green', 'green', 'red'],
        'weight': [200, 100, 150, 90, 85,95,99,102],
        'label': [0, 1, 0, 1, 1,0,0,0]}
df = pd.DataFrame(data)
df.head()
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
# feature  
onehots = pd.get_dummies(df['color'])
df_clean = df.join(onehots)
df_clean.info()
df_clean.drop('color',axis=1,inplace=True)
# Split the data into training and testing sets
X = df_clean.drop(['label', 'fruit'], axis=1)
y = df_clean['label']
random_state=0
test_size=0.3
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)

# Train the decision tree classifier
cf = DecisionTreeClassifier(random_state=0)
cf = cf.fit(X_train, y_train)

# Test the classifier on the testing data
y_pred = cf.predict(X_test)
test_accuracy= accuracy_score(y_test, y_pred)
train_accuracy=accuracy_score(y_train, cf.predict(X_train))
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Accuracy:", accuracy_score(y_train, cf.predict(X_train)))
data = {'fruit': ['apple', 'banana', 'apple', 'banana', 'banana','apple','apple','apple'],
        'color': ['red', 'yellow', 'green', 'yellow', 'yellow','green', 'green', 'red'],
        'weight': [200, 100, 150, 90, 85,95,99,102],
        'label': [0, 1, 0, 1, 1,0,0,0]}
df = pd.DataFrame(data)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
# feature  
onehots = pd.get_dummies(df['color'])
df_clean = df.join(onehots)
df_clean.info()
df_clean.drop('color',axis=1,inplace=True)
# Split the data into training and testing sets
X = df_clean.drop(['label', 'fruit'], axis=1)
y = df_clean['label']
random_state=0
test_size=0.3
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)
# Create a KNN classifier with different values of k
scaler = StandardScaler()
scaler=scaler.fit(X_train)
# standardization 
X_train_scaler = scaler.transform(X_train) 
X_test_scaler = scaler.transform(X_test) 

knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(X_train_scaler, y_train)
# Make predictions on the test data
y_pred_test= knn.predict(X_test_scaler)
y_pred_train = knn.predict(X_train_scaler)
# Calculate the accuracy of the model
test_accuracy = accuracy_score(y_test, y_pred_test)
train_accuracy = accuracy_score(y_train, y_pred_train)
print('accuracy train', train_accuracy)
print('accuracy test', test_accuracy)

Q1. What are the advantages of the decision tree?

a) Decision trees are easy to visualize

b) Less Data Preparation

c) None of the above

Correct answer: a) and b)

Q2. Choose the correct statement from below.

a) A decision tree is a graphical representation of all the possible solutions to a decision based on certain conditions.

b) A decision tree model consists of a set of rules for dividing a large homogenous population into smaller classes.

c)  decision tree model consists of a set of rules for dividing a large heterogeneous population into smaller, more homogenous (mutually exclusive) classes.

Correct answer: a) and c)

Q3. A student has a dataset with 500 data points that he want to use to train a KNN classifier. He trains 4 kNN classifiers (k={1,3,5,10}) using all the data points. Then you randomly select 300 data points from the 500, and classify them using each of the 4 classifiers.Which classifier will come out as the best one?

a) k=1

b) k=10

c) k=5

d) k=3

Correct answer: a)

Q4. Based on the following figure, identify the class of the black point if you train a K-NN algorithm with k=2.

a) Women

b) Men

Correct answer: b)

Q5. Choose the correct statement from below.

a) KNN is an instance-based algorithm, which means that it does not need to train a model before making predictions.

b) The advantages of Decision Trees include their interpretability, versatility, and feature selection capabilities.

c) KNN is a parametric algorithm, which means it can model complex relationships between input and output variables making assumptions about the underlying data distribution.

d) KNN is easy to interpret and visualize, which can be useful for understanding the decision-making process of the algorithm.

Correct answer: a) and b)

Q6. Code:
# Load the sample data
data = pd.DataFrame({'age': [23, 25, 22, 21, 24, 26, 20, 22, 19, 23, 25, 27, 21, 24, 22, 25, 26, 29, 31, 28],
                     'income': [50000, 60000, 55000, 65000, 65000, 70000, 45000, 62000, 48000, 50000, 67000, 72000, 49000, 55000, 65000, 62000, 72000, 75000, 85000, 90000],
                     'student': [0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1],
                     'credit_rating': [620, 630, 600, 675, 635, 700, 625, 650, 575, 645, 725, 675, 550, 575, 600, 650, 720, 775, 800, 850],
                     'class': [0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]})


# Split the data into training and testing sets
X = data.drop(["class"], axis=1)
y = data["class"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# scaler
scaler = StandardScaler()
scaler=scaler.fit(X_train)
# standardization 
X_train_scaler = scaler.transform(X_train) 
X_test_scaler = scaler.transform(X_test) 
# Create a KNN classifier with different values of k
k_values = [2,3,5,8]
for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_scaler, y_train)
    accuracy = knn.score(X_test_scaler, y_test)
   # print("For k =", k, "the accuracy is", accuracy)
    # Test the classifier on the testing data
    y_pred = knn.predict(X_test_scaler)
    print("Accuracy knn:",k, accuracy_score(y_test, y_pred))
# Train the logistic regression model

# Create a decision tree with different values of max_depth
d_values = [2,3,5,8]
for d in d_values:
    clf = DecisionTreeClassifier(max_depth=d, random_state=10)
    clf.fit(X_train, y_train)
    # Test the classifier on the testing data
    y_pred = clf.predict(X_test)
    print("Accuracy dt:",d, accuracy_score(y_test, y_pred))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
Your objective is to identify which models performed best in terms of the evaluation metric.

a) KNN with k=1

b) Decision tree with max_depth=2 and random_state =10

c) KNN with k=9

d) Decision tree with max_depth=5 and random_state =10

e) Decision tree with max_depth=3 and random_state =10

f) KNN with k=2

Correct answer: d)
